{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "a) Convert the text corpus into tokens."
      ],
      "metadata": {
        "id": "Xb_WdeuToDf-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "\n",
        "# Load the CSV file into a pandas dataframe\n",
        "df = pd.read_csv('/content/Corona_NLP_test.csv', encoding='latin1')\n",
        "\n",
        "# Tokenize the \"OriginalTweet\" column using NLTK\n",
        "df['OriginalTweet'] = df['OriginalTweet'].apply(lambda x: word_tokenize(x))\n",
        "\n",
        "# Print the tokenized \"OriginalTweet\" column\n",
        "print(df['OriginalTweet'])\n"
      ],
      "metadata": {
        "id": "UK2go5dgjw3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMOE6udwqJlL",
        "outputId": "03617790-b457-4c94-e584-aef0b1f3e5cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "b) Perform stop word removal."
      ],
      "metadata": {
        "id": "eCuFdCibqLh-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download stopwords if necessary\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Get the list of English stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Remove stopwords from the tokenized \"OriginalTweet\" column\n",
        "df['OriginalTweet'] = df['OriginalTweet'].apply(lambda x: [word for word in x if word.lower() not in stop_words])\n"
      ],
      "metadata": {
        "id": "DCNKd3Idm4pO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "c) Count Word frequencies"
      ],
      "metadata": {
        "id": "P_tb90-JqUCK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Create a list of all the words in the \"OriginalTweet\" column\n",
        "all_words = [word for tweet in df['OriginalTweet'] for word in tweet]\n",
        "\n",
        "# Count the frequency of each word in the list of all words\n",
        "word_freq = Counter(all_words)\n",
        "\n",
        "# Print the 10 most common words and their frequencies\n",
        "print(word_freq.most_common(10))\n"
      ],
      "metadata": {
        "id": "43p2X3N4naE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gQ795erum5wn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "d) Create word clouds."
      ],
      "metadata": {
        "id": "0k2fz2mTqcRd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a list of all the words in the \"OriginalTweet\" column\n",
        "all_words = [word for tweet in df['OriginalTweet'] for word in tweet]\n",
        "\n",
        "# Join the list of words into a single string\n",
        "text = ' '.join(all_words)\n",
        "\n",
        "# Create the word cloud object\n",
        "wordcloud = WordCloud(width=800, height=800, background_color='white').generate(text)\n",
        "\n",
        "# Plot the word cloud\n",
        "plt.figure(figsize=(8, 8), facecolor=None)\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout(pad=0)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "DOL1i4yaoLr6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}